$ train embedding, 1 pos_embed

accuracy: 0.7516

$ train embedding, 2 pos_embed

accuracy: 0.7604

$ train embedding, 2 pos_embed, loss_l2

Epoch 1, loss 6.05, acc 0.19 0.2425, time 6.93
Epoch 2, loss 4.64, acc 0.28 0.4461, time 8.07
Epoch 3, loss 3.71, acc 0.34 0.5204, time 8.11
Epoch 4, loss 3.09, acc 0.35 0.5605, time 8.10
Epoch 5, loss 2.69, acc 0.32 0.5852, time 8.12
Epoch 6, loss 2.49, acc 0.39 0.6073, time 7.95
Epoch 7, loss 2.30, acc 0.44 0.6257, time 7.91
Epoch 8, loss 2.19, acc 0.37 0.6463, time 7.91
Epoch 9, loss 1.67, acc 0.47 0.6540, time 7.88
Epoch 10, loss 1.64, acc 0.49 0.6665, time 7.88
Epoch 11, loss 1.95, acc 0.42 0.6728, time 7.88
Epoch 12, loss 1.48, acc 0.49 0.6768, time 7.93
Epoch 13, loss 1.82, acc 0.44 0.6835, time 8.51
Epoch 14, loss 1.24, acc 0.59 0.6901, time 8.52
Epoch 15, loss 1.34, acc 0.53 0.6975, time 8.22
Epoch 16, loss 1.38, acc 0.53 0.6967, time 8.97
Epoch 17, loss 1.40, acc 0.53 0.7052, time 8.53
Epoch 18, loss 1.19, acc 0.61 0.7092, time 8.61
Epoch 19, loss 1.07, acc 0.60 0.7144, time 8.06
Epoch 20, loss 1.24, acc 0.54 0.7170, time 8.03
Epoch 21, loss 1.19, acc 0.64 0.7177, time 7.91
Epoch 22, loss 1.05, acc 0.61 0.7214, time 8.21
Epoch 23, loss 0.92, acc 0.68 0.7280, time 8.18
Epoch 24, loss 1.17, acc 0.57 0.7287, time 8.03
Epoch 25, loss 1.02, acc 0.68 0.7321, time 8.33
Epoch 26, loss 0.96, acc 0.68 0.7324, time 8.10
Epoch 27, loss 0.93, acc 0.70 0.7357, time 8.17
Epoch 28, loss 0.76, acc 0.70 0.7328, time 8.01
Epoch 29, loss 0.84, acc 0.72 0.7332, time 7.87
Epoch 30, loss 0.86, acc 0.73 0.7376, time 7.89
Epoch 31, loss 0.91, acc 0.71 0.7394, time 8.03
Epoch 32, loss 0.64, acc 0.78 0.7405, time 8.15
Epoch 33, loss 0.77, acc 0.70 0.7442, time 7.88
Epoch 34, loss 0.50, acc 0.83 0.7442, time 7.97
Epoch 35, loss 0.76, acc 0.74 0.7464, time 8.14
Epoch 36, loss 0.72, acc 0.79 0.7468, time 7.95
Epoch 37, loss 0.65, acc 0.81 0.7449, time 7.96
Epoch 38, loss 0.61, acc 0.77 0.7508, time 8.19
Epoch 39, loss 0.66, acc 0.76 0.7497, time 8.22
Epoch 40, loss 0.58, acc 0.76 0.7497, time 8.14
Epoch 41, loss 0.66, acc 0.82 0.7530, time 8.22
Epoch 42, loss 0.69, acc 0.77 0.7556, time 8.26
Epoch 43, loss 0.69, acc 0.77 0.7582, time 8.30
Epoch 44, loss 0.76, acc 0.79 0.7556, time 8.71
Epoch 45, loss 0.65, acc 0.79 0.7563, time 8.03
Epoch 46, loss 0.52, acc 0.82 0.7589, time 8.03
Epoch 47, loss 0.52, acc 0.86 0.7560, time 8.29
Epoch 48, loss 0.44, acc 0.83 0.7604, time 8.64
Epoch 49, loss 0.45, acc 0.80 0.7611, time 9.14
Epoch 50, loss 0.43, acc 0.80 0.7641, time 8.54
accuracy: 0.7641

0.7611



2017-12-06T13:12:19.902268: epoch 1.0, loss 8.02038, acc 0.117 acc 0.339713
2017-12-06T13:12:22.016497: epoch 2.0, loss 5.21093, acc 0.20525 acc 0.459698
2017-12-06T13:12:24.007700: epoch 3.0, loss 3.83102, acc 0.294375 acc 0.521899
2017-12-06T13:12:26.006858: epoch 4.0, loss 3.05821, acc 0.356125 acc 0.565697
2017-12-06T13:12:27.995421: epoch 5.0, loss 2.49265, acc 0.424625 acc 0.593669
2017-12-06T13:12:29.979631: epoch 6.0, loss 2.17564, acc 0.46925 acc 0.610232
2017-12-06T13:12:32.008778: epoch 7.0, loss 1.92131, acc 0.5045 acc 0.624586
2017-12-06T13:12:34.063499: epoch 8.0, loss 1.74082, acc 0.531375 acc 0.643357
2017-12-06T13:12:36.106583: epoch 9.0, loss 1.60087, acc 0.556125 acc 0.652926
2017-12-06T13:12:38.135145: epoch 10.0, loss 1.48861, acc 0.5865 acc 0.666544
2017-12-06T13:12:40.161166: epoch 11.0, loss 1.39328, acc 0.597875 acc 0.673169
2017-12-06T13:12:42.205349: epoch 12.0, loss 1.32584, acc 0.622375 acc 0.680898
2017-12-06T13:12:44.248951: epoch 13.0, loss 1.28461, acc 0.637375 acc 0.678322
2017-12-06T13:12:46.268522: epoch 14.0, loss 1.2194, acc 0.65125 acc 0.683106
2017-12-06T13:12:48.304760: epoch 15.0, loss 1.16311, acc 0.662125 acc 0.688259
2017-12-06T13:12:50.354910: epoch 16.0, loss 1.12328, acc 0.674125 acc 0.695252
2017-12-06T13:12:52.389893: epoch 17.0, loss 1.11418, acc 0.6815 acc 0.696356
2017-12-06T13:12:54.428253: epoch 18.0, loss 1.06757, acc 0.696 acc 0.697092
2017-12-06T13:12:56.466865: epoch 19.0, loss 1.0313, acc 0.700375 acc 0.70519
2017-12-06T13:12:58.518222: epoch 20.0, loss 1.01061, acc 0.709 acc 0.708502
2017-12-06T13:13:00.552163: epoch 21.0, loss 0.992967, acc 0.7135 acc 0.709974
2017-12-06T13:13:02.593586: epoch 22.0, loss 0.968373, acc 0.719875 acc 0.723224
2017-12-06T13:13:04.635490: epoch 23.0, loss 0.947148, acc 0.723 acc 0.724696
2017-12-06T13:13:06.678739: epoch 24.0, loss 0.920334, acc 0.736875 acc 0.722856
2017-12-06T13:13:08.722512: epoch 25.0, loss 0.897015, acc 0.7405 acc 0.726905
2017-12-06T13:13:10.758756: epoch 26.0, loss 0.881229, acc 0.751 acc 0.729481
2017-12-06T13:13:12.795776: epoch 27.0, loss 0.8638, acc 0.75725 acc 0.726537
2017-12-06T13:13:14.830749: epoch 28.0, loss 0.838739, acc 0.7655 acc 0.733898
2017-12-06T13:13:16.865012: epoch 29.0, loss 0.817999, acc 0.766875 acc 0.732057
2017-12-06T13:13:18.899714: epoch 30.0, loss 0.798188, acc 0.774625 acc 0.73721
2017-12-06T13:13:20.953184: epoch 31.0, loss 0.788661, acc 0.777 acc 0.734634
2017-12-06T13:13:22.978232: epoch 32.0, loss 0.758735, acc 0.786125 acc 0.740155
2017-12-06T13:13:25.022069: epoch 33.0, loss 0.748659, acc 0.790125 acc 0.735738
2017-12-06T13:13:27.078214: epoch 34.0, loss 0.736075, acc 0.800125 acc 0.73537
2017-12-06T13:13:29.126368: epoch 35.0, loss 0.725236, acc 0.796 acc 0.736106
2017-12-06T13:13:31.175093: epoch 36.0, loss 0.699411, acc 0.811875 acc 0.73721
2017-12-06T13:13:33.207219: epoch 37.0, loss 0.688934, acc 0.813 acc 0.740891
2017-12-06T13:13:35.248259: epoch 38.0, loss 0.6603, acc 0.824125 acc 0.741995
2017-12-06T13:13:37.286386: epoch 39.0, loss 0.651408, acc 0.823125 acc 0.746043
2017-12-06T13:13:39.328688: epoch 40.0, loss 0.635034, acc 0.831875 acc 0.747148
2017-12-06T13:13:41.384496: epoch 41.0, loss 0.620871, acc 0.839375 acc 0.752668
2017-12-06T13:13:43.412687: epoch 42.0, loss 0.605621, acc 0.84575 acc 0.750092
2017-12-06T13:13:45.442388: epoch 43.0, loss 0.601074, acc 0.842875 acc 0.74862
2017-12-06T13:13:47.483276: epoch 44.0, loss 0.583193, acc 0.84825 acc 0.754141
2017-12-06T13:13:49.505831: epoch 45.0, loss 0.566848, acc 0.85625 acc 0.751196
2017-12-06T13:13:51.564679: epoch 46.0, loss 0.561003, acc 0.85775 acc 0.755613
2017-12-06T13:13:53.5985.0,0.776963953: epoch 47.0, loss 0.537976, acc 0.866 acc 0.753036
2017-12-06T13:13:55.643590: epoch 48.0, loss 0.531, acc 0.867375 acc 0.755613
2017-12-06T13:13:57.679820: epoch 49.0, loss 0.530986, acc 0.86375 acc 0.758925
2017-12-06T13:13:59.732731: epoch 50.0, loss 0.51478, acc 0.874625 acc 0.758189
2017-12-06T13:14:01.797705: epoch 51.0, loss 0.506944, acc 0.875875 acc 0.760029
2017-12-06T13:14:03.844401: epoch 52.0, loss 0.479886, acc 0.882625 acc 0.760029
2017-12-06T13:14:05.901457: epoch 53.0, loss 0.486397, acc 0.88275 acc 0.758189
2017-12-06T13:14:07.961250: epoch 54.0, loss 0.475795, acc 0.885875 acc 0.76371
2017-12-06T13:14:10.008531: epoch 55.0, loss 0.459337, acc 0.89525 acc 0.757821
2017-12-06T13:14:12.069939: epoch 56.0, loss 0.450118, acc 0.893625 acc 0.759293
2017-12-06T13:14:14.142195: epoch 57.0, loss 0.450393, acc 0.89275 acc 0.765918
2017-12-06T13:14:16.212258: epoch 58.0, loss 0.432321, acc 0.90475 acc 0.766654
2017-12-06T13:14:18.253138: epoch 59.0, loss 0.418125, acc 0.906375 acc 0.764814
2017-12-06T13:14:20.314556: epoch 60.0, loss 0.406918, acc 0.9105 acc 0.764446
2017-12-06T13:14:22.371136: epoch 61.0, loss 0.40216, acc 0.90875 acc 0.76371
2017-12-06T13:14:24.434587: epoch 62.0, loss 0.400859, acc 0.910875 acc 0.768495
2017-12-06T13:14:26.484669: epoch 63.0, loss 0.392737, acc 0.91375 acc 0.765182
2017-12-06T13:14:28.530336: epoch 64.0, loss 0.383606, acc 0.916375 acc 0.764078
2017-12-06T13:14:30.587850: epoch 65.0, loss 0.377736, acc 0.91925 acc 0.76739
2017-12-06T13:14:32.641307: epoch 66.0, loss 0.376472, acc 0.9185 acc 0.768127
2017-12-06T13:14:34.706441: epoch 67.0, loss 0.356782, acc 0.924 acc 0.763342
2017-12-06T13:14:36.740190: epoch 68.0, loss 0.351782, acc 0.92475 acc 0.764078
2017-12-06T13:14:38.793685: epoch 69.0, loss 0.35649, acc 0.926875 acc 0.76187
2017-12-06T13:14:40.838808: epoch 70.0, loss 0.345197, acc 0.931 acc 0.767022
2017-12-06T13:14:42.900960: epoch 71.0, loss 0.340959, acc 0.928375 acc 0.767022
2017-12-06T13:14:44.944895: epoch 72.0, loss 0.328758, acc 0.936 acc 0.76555
2017-12-06T13:14:47.011526: epoch 73.0, loss 0.319224, acc 0.93625 acc 0.767022
2017-12-06T13:14:49.064071: epoch 74.0, loss 0.324008, acc 0.93525 acc 0.768863
2017-12-06T13:14:51.121247: epoch 75.0, loss 0.313333, acc 0.9385 acc 0.768495
2017-12-06T13:14:53.170415: epoch 76.0, loss 0.298908, acc 0.944375 acc 0.767759
2017-12-06T13:14:55.228219: epoch 77.0, loss 0.299541, acc 0.9405 acc 0.769599
2017-12-06T13:14:57.288650: epoch 78.0, loss 0.305413, acc 0.938875 acc 0.773279
2017-12-06T13:14:59.335258: epoch 79.0, loss 0.299061, acc 0.943 acc 0.771439
2017-12-06T13:15:01.396019: epoch 80.0, loss 0.290207, acc 0.945875 acc 0.771071
2017-12-06T13:15:03.469210: epoch 81.0, loss 0.280889, acc 0.950125 acc 0.764078
2017-12-06T13:15:05.524659: epoch 82.0, loss 0.285329, acc 0.945375 acc 0.771439
2017-12-06T13:15:07.594272: epoch 83.0, loss 0.276878, acc 0.949375 acc 0.769599
2017-12-06T13:15:09.659905: epoch 84.0, loss 0.28164, acc 0.947 acc 0.769967
2017-12-06T13:15:11.723867: epoch 85.0, loss 0.26283, acc 0.953125 acc 0.77696
2017-12-06T13:15:13.777244: epoch 86.0, loss 0.259493, acc 0.9535 acc 0.769967
2017-12-06T13:15:15.842235: epoch 87.0, loss 0.264578, acc 0.94975 acc 0.774015
2017-12-06T13:15:17.900947: epoch 88.0, loss 0.259292, acc 0.9535 acc 0.767759
2017-12-06T13:15:19.940728: epoch 89.0, loss 0.251375, acc 0.954875 acc 0.766654
2017-12-06T13:15:21.972599: epoch 90.0, loss 0.253155, acc 0.953125 acc 0.764814
2017-12-06T13:15:24.031988: epoch 91.0, loss 0.24537, acc 0.954875 acc 0.774383
2017-12-06T13:15:26.086168: epoch 92.0, loss 0.243811, acc 0.956 acc 0.771439
2017-12-06T13:15:28.141523: epoch 93.0, loss 0.23534, acc 0.961125 acc 0.769967
2017-12-06T13:15:30.206015: epoch 94.0, loss 0.236593, acc 0.95825 acc 0.768863
2017-12-06T13:15:32.272770: epoch 95.0, loss 0.239393, acc 0.957875 acc 0.766654
2017-12-06T13:15:34.323774: epoch 96.0, loss 0.232149, acc 0.958375 acc 0.774383
2017-12-06T13:15:36.381286: epoch 97.0, loss 0.221432, acc 0.963875 acc 0.771439
2017-12-06T13:15:38.445485: epoch 98.0, loss 0.226152, acc 0.96 acc 0.773647
2017-12-06T13:15:40.494963: epoch 99.0, loss 0.214748, acc 0.967875 acc 0.771439
2017-12-06T13:15:42.543592: epoch 100.0, loss 0.221048, acc 0.96525 acc 0.773647
2017-12-06T13:15:44.600687: epoch 101.0, loss 0.216585, acc 0.964625 acc 0.768863
2017-12-06T13:15:46.672788: epoch 102.0, loss 0.207547, acc 0.96475 acc 0.769599
2017-12-06T13:15:48.720024: epoch 103.0, loss 0.202008, acc 0.96725 acc 0.772543
2017-12-06T13:15:50.782257: epoch 104.0, loss 0.20382, acc 0.967875 acc 0.768863
2017-12-06T13:15:52.830933: epoch 105.0, loss 0.201699, acc 0.969 acc 0.768495
2017-12-06T13:15:54.900507: epoch 106.0, loss 0.207076, acc 0.964875 acc 0.769231
2017-12-06T13:15:56.960287: epoch 107.0, loss 0.189853, acc 0.973375 acc 0.770703
2017-12-06T13:15:59.016328: epoch 108.0, loss 0.196869, acc 0.97 acc 0.771439
2017-12-06T13:16:01.069804: epoch 109.0, loss 0.195065, acc 0.970125 acc 0.76739
2017-12-06T13:16:03.133055: epoch 110.0, loss 0.188959, acc 0.970875 acc 0.770335
2017-12-06T13:16:05.204854: epoch 111.0, loss 0.185942, acc 0.97125 acc 0.76555
2017-12-06T13:16:07.270858: epoch 112.0, loss 0.182113, acc 0.9735 acc 0.772911
2017-12-06T13:16:09.328522: epoch 113.0, loss 0.188085, acc 0.9705 acc 0.768863
2017-12-06T13:16:11.382050: epoch 114.0, loss 0.18681, acc 0.97175 acc 0.768495
2017-12-06T13:16:13.437027: epoch 115.0, loss 0.171837, acc 0.976875 acc 0.768127
2017-12-06T13:16:15.498223: epoch 116.0, loss 0.1798, acc 0.97375 acc 0.765918
2017-12-06T13:16:17.553078: epoch 117.0, loss 0.179118, acc 0.975 acc 0.768863
2017-12-06T13:16:19.604418: epoch 118.0, loss 0.174284, acc 0.975625 acc 0.768863
2017-12-06T13:16:21.665339: epoch 119.0, loss 0.168435, acc 0.978625 acc 0.768495
2017-12-06T13:16:23.711824: epoch 120.0, loss 0.170968, acc 0.97525 acc 0.767022
2017-12-06T13:16:25.758606: epoch 121.0, loss 0.164665, acc 0.975875 acc 0.766286
2017-12-06T13:16:27.795338: epoch 122.0, loss 0.163484, acc 0.97725 acc 0.769967
2017-12-06T13:16:29.823278: epoch 123.0, loss 0.162461, acc 0.977875 acc 0.767759
2017-12-06T13:16:31.875711: epoch 124.0, loss 0.158088, acc 0.9795 acc 0.768495
2017-12-06T13:16:33.931369: epoch 125.0, loss 0.158335, acc 0.97775 acc 0.769599
2017-12-06T13:16:35.989470: epoch 126.0, loss 0.160946, acc 0.976375 acc 0.765918
2017-12-06T13:16:38.045231: epoch 127.0, loss 0.154717, acc 0.978125 acc 0.772175
2017-12-06T13:16:40.116383: epoch 128.0, loss 0.161104, acc 0.97775 acc 0.76739
2017-12-06T13:16:42.186603: epoch 129.0, loss 0.154635, acc 0.977875 acc 0.76739
2017-12-06T13:16:44.229252: epoch 130.0, loss 0.159365, acc 0.977375 acc 0.773647
2017-12-06T13:16:46.288465: epoch 131.0, loss 0.147002, acc 0.983375 acc 0.770703
2017-12-06T13:16:48.344951: epoch 132.0, loss 0.148113, acc 0.981375 acc 0.767759
2017-12-06T13:16:50.389335: epoch 133.0, loss 0.149837, acc 0.97875 acc 0.761134
2017-12-06T13:16:52.426742: epoch 134.0, loss 0.146484, acc 0.980875 acc 0.771439
2017-12-06T13:16:54.465053: epoch 135.0, loss 0.145072, acc 0.97975 acc 0.770335
2017-12-06T13:16:56.516452: epoch 136.0, loss 0.144512, acc 0.981375 acc 0.76739
2017-12-06T13:16:58.568102: epoch 137.0, loss 0.137904, acc 0.9835 acc 0.771071
2017-12-06T13:17:00.622081: epoch 138.0, loss 0.144045, acc 0.979875 acc 0.768127
2017-12-06T13:17:02.672825: epoch 139.0, loss 0.139639, acc 0.98125 acc 0.764446
2017-12-06T13:17:04.712960: epoch 140.0, loss 0.136979, acc 0.981375 acc 0.768863
2017-12-06T13:17:06.744528: epoch 141.0, loss 0.139607, acc 0.98 acc 0.767759
2017-12-06T13:17:08.787341: epoch 142.0, loss 0.137371, acc 0.98125 acc 0.768127
2017-12-06T13:17:10.839518: epoch 143.0, loss 0.143348, acc 0.98125 acc 0.76555
2017-12-06T13:17:12.886104: epoch 144.0, loss 0.1335, acc 0.98325 acc 0.767759
2017-12-06T13:17:14.920667: epoch 145.0, loss 0.137114, acc 0.982 acc 0.771071
2017-12-06T13:17:16.949024: epoch 146.0, loss 0.129519, acc 0.9845 acc 0.76739
2017-12-06T13:17:18.980605: epoch 147.0, loss 0.13411, acc 0.981375 acc 0.76187
2017-12-06T13:17:21.001968: epoch 148.0, loss 0.12731, acc 0.983875 acc 0.76371
2017-12-06T13:17:23.043505: epoch 149.0, loss 0.13186, acc 0.981625 acc 0.76371
2017-12-06T13:17:25.073097: epoch 150.0, loss 0.127684, acc 0.984125 acc 0.765182
2017-12-06T13:17:27.106328: epoch 151.0, loss 0.125002, acc 0.98475 acc 0.765918
2017-12-06T13:17:29.140967: epoch 152.0, loss 0.125638, acc 0.98525 acc 0.767759
2017-12-06T13:17:31.174513: epoch 153.0, loss 0.122511, acc 0.983625 acc 0.764446
2017-12-06T13:17:33.209764: epoch 154.0, loss 0.119794, acc 0.98525 acc 0.764814
2017-12-06T13:17:35.233723: epoch 155.0, loss 0.121922, acc 0.984125 acc 0.763342
2017-12-06T13:17:37.265019: epoch 156.0, loss 0.121594, acc 0.985125 acc 0.76371
2017-12-06T13:17:39.299697: epoch 157.0, loss 0.118672, acc 0.98425 acc 0.765918
2017-12-06T13:17:41.341221: epoch 158.0, loss 0.115452, acc 0.986625 acc 0.76739
2017-12-06T13:17:43.384110: epoch 159.0, loss 0.114149, acc 0.986 acc 0.766286
2017-12-06T13:17:45.421913: epoch 160.0, loss 0.119383, acc 0.9835 acc 0.76555
2017-12-06T13:17:47.471541: epoch 161.0, loss 0.113057, acc 0.98725 acc 0.765918
2017-12-06T13:17:49.499422: epoch 162.0, loss 0.110007, acc 0.9865 acc 0.768495
2017-12-06T13:17:51.546486: epoch 163.0, loss 0.113025, acc 0.98725 acc 0.762606
2017-12-06T13:17:53.590321: epoch 164.0, loss 0.107762, acc 0.98825 acc 0.763342
2017-12-06T13:17:55.632939: epoch 165.0, loss 0.113197, acc 0.98575 acc 0.76187
2017-12-06T13:17:57.675801: epoch 166.0, loss 0.115642, acc 0.98525 acc 0.765182
2017-12-06T13:17:59.717031: epoch 167.0, loss 0.108806, acc 0.9875 acc 0.768127
2017-12-06T13:18:01.752252: epoch 168.0, loss 0.108373, acc 0.986625 acc 0.764814
2017-12-06T13:18:03.792286: epoch 169.0, loss 0.109483, acc 0.986125 acc 0.765918
2017-12-06T13:18:05.827990: epoch 170.0, loss 0.109762, acc 0.988125 acc 0.769967
2017-12-06T13:18:07.852850: epoch 171.0, loss 0.107831, acc 0.987625 acc 0.76555
2017-12-06T13:18:09.890526: epoch 172.0, loss 0.1054, acc 0.9885 acc 0.76739
2017-12-06T13:18:11.931191: epoch 173.0, loss 0.108308, acc 0.986875 acc 0.764814
2017-12-06T13:18:13.975316: epoch 174.0, loss 0.105442, acc 0.986125 acc 0.76555
2017-12-06T13:18:15.987240: epoch 175.0, loss 0.104679, acc 0.986875 acc 0.760766
2017-12-06T13:18:18.009924: epoch 176.0, loss 0.101803, acc 0.98825 acc 0.762606
2017-12-06T13:18:20.020799: epoch 177.0, loss 0.102602, acc 0.988125 acc 0.764814
2017-12-06T13:18:22.042527: epoch 178.0, loss 0.100407, acc 0.9895 acc 0.758925
2017-12-06T13:18:24.079108: epoch 179.0, loss 0.102942, acc 0.987625 acc 0.762238
2017-12-06T13:18:26.100030: epoch 180.0, loss 0.100555, acc 0.98725 acc 0.762974
2017-12-06T13:18:28.114702: epoch 181.0, loss 0.0998572, acc 0.988 acc 0.767759
2017-12-06T13:18:30.132152: epoch 182.0, loss 0.101969, acc 0.9875 acc 0.76187
2017-12-06T13:18:32.171008: epoch 183.0, loss 0.0987393, acc 0.988375 acc 0.76555
2017-12-06T13:18:34.180570: epoch 184.0, loss 0.0956503, acc 0.9895 acc 0.768863
2017-12-06T13:18:36.195770: epoch 185.0, loss 0.0957487, acc 0.989375 acc 0.762238
2017-12-06T13:18:38.207293: epoch 186.0, loss 0.0925463, acc 0.989375 acc 0.761502
2017-12-06T13:18:40.223110: epoch 187.0, loss 0.0971171, acc 0.9875 acc 0.766286
2017-12-06T13:18:42.244655: epoch 188.0, loss 0.0955563, acc 0.98825 acc 0.768495
2017-12-06T13:18:44.259451: epoch 189.0, loss 0.0960079, acc 0.989125 acc 0.761134
2017-12-06T13:18:46.252531: epoch 190.0, loss 0.0918301, acc 0.99025 acc 0.765182
2017-12-06T13:18:48.255661: epoch 191.0, loss 0.0939242, acc 0.989875 acc 0.765918
2017-12-06T13:18:50.255104: epoch 192.0, loss 0.0921254, acc 0.989625 acc 0.76555
2017-12-06T13:18:52.251418: epoch 193.0, loss 0.0878699, acc 0.9905 acc 0.767022
2017-12-06T13:18:54.271148: epoch 194.0, loss 0.0887109, acc 0.990375 acc 0.770335
2017-12-06T13:18:56.274064: epoch 195.0, loss 0.0907183, acc 0.988875 acc 0.769231
2017-12-06T13:18:58.292767: epoch 196.0, loss 0.0858553, acc 0.99 acc 0.769599
2017-12-06T13:19:00.300399: epoch 197.0, loss 0.0887674, acc 0.9895 acc 0.769599
2017-12-06T13:19:02.312580: epoch 198.0, loss 0.0916476, acc 0.98875 acc 0.767022
2017-12-06T13:19:04.320531: epoch 199.0, loss 0.0910088, acc 0.989 acc 0.758557
2017-12-06T13:19:06.332987: epoch 200.0, loss 0.0873541, acc 0.99025 acc 0.763342

best epoch 85, 0.77696


$ don't train embedding, 2 pos_embed, loss_l2

Epoch 50, loss 1.04, acc 0.62 0.7236, time 7.52
Done training, best_step: 3680, best_acc: 0.7276
duration: 0.11 hours

epoch 50

re.sub("\d+", "0", w) 0.7597
no sub                0.7604 0.7608
clear_str             0.7560

entity.first           0.7582
entity.second          0.7575  

pad -1                 0.7560
pad -1 clear_str       0.7538
epoch 200
best: 0.77696

$ train embedding, 2 pos_embed, loss_l2

Epoch 1, loss 6.10, acc 0.16 0.2425, time 6.88
Epoch 2, loss 4.71, acc 0.20 0.4015, time 7.83
Epoch 3, loss 3.53, acc 0.28 0.4884, time 7.81
Epoch 4, loss 3.34, acc 0.29 0.5469, time 7.82
Epoch 5, loss 2.50, acc 0.39 0.5812, time 7.85
Epoch 6, loss 2.24, acc 0.37 0.6036, time 7.85
Epoch 7, loss 2.24, acc 0.37 0.6169, time 7.79
Epoch 8, loss 2.06, acc 0.46 0.6360, time 7.80
Epoch 9, loss 2.23, acc 0.39 0.6478, time 7.79
Epoch 10, loss 1.65, acc 0.43 0.6581, time 7.84
Epoch 11, loss 1.59, acc 0.50 0.6702, time 8.16
Epoch 12, loss 1.59, acc 0.47 0.6802, time 8.00
Epoch 13, loss 1.25, acc 0.55 0.6813, time 7.85
Epoch 14, loss 1.36, acc 0.46 0.6857, time 7.83
Epoch 15, loss 1.30, acc 0.48 0.6927, time 7.85
Epoch 16, loss 1.06, acc 0.64 0.6971, time 7.83
Epoch 17, loss 1.29, acc 0.58 0.7004, time 7.83
Epoch 18, loss 1.13, acc 0.66 0.7056, time 7.84
Epoch 19, loss 0.99, acc 0.67 0.7089, time 7.84
Epoch 20, loss 1.12, acc 0.62 0.7114, time 7.84
Epoch 21, loss 1.01, acc 0.59 0.7129, time 7.82
Epoch 22, loss 1.18, acc 0.58 0.7166, time 7.87
Epoch 23, loss 1.03, acc 0.73 0.7144, time 7.83
Epoch 24, loss 0.80, acc 0.74 0.7203, time 7.70
Epoch 25, loss 1.04, acc 0.60 0.7236, time 7.82
Epoch 26, loss 0.87, acc 0.70 0.7269, time 8.01
Epoch 27, loss 0.95, acc 0.70 0.7280, time 8.08
Epoch 28, loss 1.02, acc 0.66 0.7310, time 8.33
Epoch 29, loss 0.67, acc 0.77 0.7379, time 8.42
Epoch 30, loss 0.73, acc 0.76 0.7427, time 8.57
Epoch 31, loss 0.91, acc 0.69 0.7431, time 8.76
Epoch 32, loss 0.74, acc 0.78 0.7442, time 8.60
Epoch 33, loss 0.67, acc 0.77 0.7457, time 9.06
Epoch 34, loss 0.69, acc 0.77 0.7405, time 9.39
Epoch 35, loss 0.79, acc 0.74 0.7442, time 9.56
Epoch 36, loss 0.63, acc 0.80 0.7471, time 9.55
Epoch 37, loss 0.77, acc 0.71 0.7512, time 9.44
Epoch 38, loss 0.63, acc 0.76 0.7575, time 9.47
Epoch 39, loss 0.73, acc 0.72 0.7530, time 9.44
Epoch 40, loss 0.58, acc 0.83 0.7556, time 9.32
Epoch 41, loss 0.58, acc 0.78 0.7589, time 9.30
Epoch 42, loss 0.55, acc 0.82 0.7575, time 9.38
Epoch 43, loss 0.62, acc 0.76 0.7600, time 9.23
Epoch 44, loss 0.57, acc 0.76 0.7578, time 9.38
Epoch 45, loss 0.62, acc 0.83 0.7593, time 11.03
Epoch 46, loss 0.53, acc 0.82 0.7549, time 9.22
Epoch 47, loss 0.35, acc 0.85 0.7586, time 9.18
Epoch 48, loss 0.40, acc 0.83 0.7626, time 9.27
Epoch 49, loss 0.45, acc 0.84 0.7604, time 9.39
Epoch 50, loss 0.37, acc 0.84 0.7611, time 9.29
Epoch 51, loss 0.55, acc 0.84 0.7633, time 9.14
Epoch 52, loss 0.50, acc 0.81 0.7633, time 9.19
Epoch 53, loss 0.65, acc 0.76 0.7630, time 9.05
Epoch 54, loss 0.35, acc 0.88 0.7674, time 9.07
Epoch 55, loss 0.37, acc 0.90 0.7674, time 9.06
Epoch 56, loss 0.40, acc 0.86 0.7630, time 9.05
Epoch 57, loss 0.42, acc 0.83 0.7641, time 9.04
Epoch 58, loss 0.22, acc 0.92 0.7615, time 9.08
Epoch 59, loss 0.38, acc 0.85 0.7571, time 9.10
Epoch 60, loss 0.50, acc 0.83 0.7615, time 9.01
Epoch 61, loss 0.38, acc 0.88 0.7582, time 9.04
Epoch 62, loss 0.41, acc 0.85 0.7611, time 8.94
Epoch 63, loss 0.33, acc 0.89 0.7633, time 9.25
Epoch 64, loss 0.39, acc 0.85 0.7667, time 9.28
Epoch 65, loss 0.35, acc 0.91 0.7663, time 9.07
Epoch 66, loss 0.31, acc 0.90 0.7597, time 9.20
Epoch 67, loss 0.30, acc 0.94 0.7656, time 10.92
Epoch 68, loss 0.31, acc 0.91 0.7644, time 9.34
Epoch 69, loss 0.33, acc 0.90 0.7637, time 9.11
Epoch 70, loss 0.28, acc 0.93 0.7659, time 9.35
Epoch 71, loss 0.19, acc 0.94 0.7633, time 9.25
Epoch 72, loss 0.26, acc 0.93 0.7667, time 11.11
Epoch 73, loss 0.19, acc 0.93 0.7659, time 9.38
Epoch 74, loss 0.23, acc 0.93 0.7692, time 9.25
Epoch 75, loss 0.34, acc 0.89 0.7667, time 9.35
Epoch 76, loss 0.23, acc 0.90 0.7696, time 9.21
Epoch 77, loss 0.23, acc 0.93 0.7600, time 9.39
Epoch 78, loss 0.20, acc 0.93 0.7681, time 9.19
Epoch 79, loss 0.20, acc 0.93 0.7652, time 9.33
Epoch 80, loss 0.18, acc 0.90 0.7648, time 11.13
Epoch 81, loss 0.19, acc 0.94 0.7644, time 8.89
Epoch 82, loss 0.12, acc 0.96 0.7619, time 9.02
Epoch 83, loss 0.16, acc 0.94 0.7630, time 9.07
Epoch 84, loss 0.22, acc 0.92 0.7641, time 9.03
Epoch 85, loss 0.28, acc 0.92 0.7626, time 9.09
Epoch 86, loss 0.19, acc 0.94 0.7619, time 8.94
Epoch 87, loss 0.22, acc 0.93 0.7637, time 9.12
Epoch 88, loss 0.23, acc 0.94 0.7622, time 9.16
Epoch 89, loss 0.17, acc 0.92 0.7667, time 9.05
Epoch 90, loss 0.19, acc 0.93 0.7630, time 9.07
Epoch 91, loss 0.27, acc 0.89 0.7600, time 9.04
Epoch 92, loss 0.15, acc 0.94 0.7615, time 9.05
Epoch 93, loss 0.11, acc 0.96 0.7633, time 10.61
Epoch 94, loss 0.06, acc 0.99 0.7633, time 9.34
Epoch 95, loss 0.11, acc 0.97 0.7630, time 9.09
Epoch 96, loss 0.19, acc 0.92 0.7659, time 9.13
Epoch 97, loss 0.19, acc 0.93 0.7644, time 9.22
Epoch 98, loss 0.12, acc 0.96 0.7604, time 9.17
Epoch 99, loss 0.10, acc 0.96 0.7611, time 9.23
Epoch 100, loss 0.25, acc 0.92 0.7644, time 9.31
Epoch 101, loss 0.10, acc 0.96 0.7593, time 9.20
Epoch 102, loss 0.18, acc 0.95 0.7641, time 9.37
Epoch 103, loss 0.19, acc 0.92 0.7641, time 9.22
Epoch 104, loss 0.07, acc 0.99 0.7652, time 9.20
Epoch 105, loss 0.11, acc 0.96 0.7681, time 9.26
Epoch 106, loss 0.10, acc 0.96 0.7656, time 9.32
Epoch 107, loss 0.09, acc 0.96 0.7641, time 9.28
Epoch 108, loss 0.10, acc 0.94 0.7637, time 9.31
Epoch 109, loss 0.18, acc 0.95 0.7722, time 9.29
Epoch 110, loss 0.06, acc 0.98 0.7685, time 9.35
Epoch 111, loss 0.08, acc 0.96 0.7659, time 9.26
Epoch 112, loss 0.17, acc 0.94 0.7663, time 9.27
Epoch 113, loss 0.08, acc 0.98 0.7696, time 9.18
Epoch 114, loss 0.16, acc 0.95 0.7681, time 9.25
Epoch 115, loss 0.05, acc 0.98 0.7685, time 9.12
Epoch 116, loss 0.15, acc 0.96 0.7670, time 8.99
Epoch 117, loss 0.15, acc 0.95 0.7674, time 9.13
Epoch 118, loss 0.10, acc 0.97 0.7663, time 10.96
Epoch 119, loss 0.10, acc 0.96 0.7700, time 9.09
Epoch 120, loss 0.09, acc 0.96 0.7700, time 9.15
Epoch 121, loss 0.12, acc 0.97 0.7656, time 9.20
Epoch 122, loss 0.18, acc 0.93 0.7674, time 9.12
Epoch 123, loss 0.09, acc 0.98 0.7678, time 10.45
Epoch 124, loss 0.09, acc 0.96 0.7685, time 9.38
Epoch 125, loss 0.14, acc 0.96 0.7637, time 8.85
Epoch 126, loss 0.06, acc 0.98 0.7615, time 9.02
Epoch 127, loss 0.09, acc 0.97 0.7619, time 9.14
Epoch 128, loss 0.11, acc 0.96 0.7608, time 9.03
Epoch 129, loss 0.09, acc 0.96 0.7622, time 8.91
Epoch 130, loss 0.03, acc 0.99 0.7622, time 9.06
Epoch 131, loss 0.04, acc 0.99 0.7560, time 9.25
Epoch 132, loss 0.12, acc 0.95 0.7615, time 9.27
Epoch 133, loss 0.07, acc 0.97 0.7641, time 9.30
Epoch 134, loss 0.01, acc 1.00 0.7633, time 9.10
Epoch 135, loss 0.06, acc 0.98 0.7622, time 8.84
Epoch 136, loss 0.07, acc 0.98 0.7615, time 9.05
Epoch 137, loss 0.02, acc 1.00 0.7586, time 9.09
Epoch 138, loss 0.09, acc 0.96 0.7630, time 9.08
Epoch 139, loss 0.18, acc 0.94 0.7656, time 10.89
Epoch 140, loss 0.08, acc 0.98 0.7652, time 9.05
Epoch 141, loss 0.13, acc 0.97 0.7641, time 9.03
Epoch 142, loss 0.03, acc 0.99 0.7648, time 9.06
Epoch 143, loss 0.12, acc 0.94 0.7637, time 9.02
Epoch 144, loss 0.08, acc 0.95 0.7648, time 8.46
Epoch 145, loss 0.04, acc 0.99 0.7593, time 8.71
Epoch 146, loss 0.10, acc 0.97 0.7630, time 8.25
Epoch 147, loss 0.03, acc 1.00 0.7563, time 7.96
Epoch 148, loss 0.12, acc 0.96 0.7648, time 7.85
Epoch 149, loss 0.11, acc 0.95 0.7652, time 8.22
Epoch 150, loss 0.04, acc 0.98 0.7663, time 8.54
Epoch 151, loss 0.04, acc 0.98 0.7696, time 7.84
Epoch 152, loss 0.11, acc 0.98 0.7626, time 7.77
Epoch 153, loss 0.08, acc 0.97 0.7670, time 8.64
Epoch 154, loss 0.06, acc 0.96 0.7626, time 8.91
Epoch 155, loss 0.04, acc 0.98 0.7615, time 8.27
Epoch 156, loss 0.07, acc 0.97 0.7597, time 8.51
Epoch 157, loss 0.12, acc 0.97 0.7622, time 8.33
Epoch 158, loss 0.07, acc 0.97 0.7611, time 7.97
Epoch 159, loss 0.02, acc 0.99 0.7641, time 8.57
Epoch 160, loss 0.06, acc 0.96 0.7604, time 8.70
Epoch 161, loss 0.07, acc 0.99 0.7619, time 8.38
Epoch 162, loss 0.01, acc 1.00 0.7652, time 8.62
Epoch 163, loss 0.07, acc 0.96 0.7622, time 9.73
Epoch 164, loss 0.02, acc 0.99 0.7644, time 8.53
Epoch 165, loss 0.01, acc 1.00 0.7659, time 8.73
Epoch 166, loss 0.01, acc 1.00 0.7652, time 8.12
Epoch 167, loss 0.03, acc 0.99 0.7652, time 8.01
Epoch 168, loss 0.02, acc 0.99 0.7667, time 7.90
Epoch 169, loss 0.12, acc 0.97 0.7648, time 7.85
Epoch 170, loss 0.09, acc 0.96 0.7633, time 8.41
Epoch 171, loss 0.01, acc 1.00 0.7637, time 9.04
Epoch 172, loss 0.01, acc 1.00 0.7593, time 7.83
Epoch 173, loss 0.04, acc 0.99 0.7578, time 7.83
Epoch 174, loss 0.05, acc 0.98 0.7578, time 7.84
Epoch 175, loss 0.01, acc 1.00 0.7589, time 7.85
Epoch 176, loss 0.05, acc 0.97 0.7608, time 7.82
Epoch 177, loss 0.03, acc 0.99 0.7593, time 7.85
Epoch 178, loss 0.12, acc 0.97 0.7633, time 7.83
Epoch 179, loss 0.02, acc 0.99 0.7637, time 7.84
Epoch 180, loss 0.01, acc 1.00 0.7622, time 7.82
Epoch 181, loss 0.03, acc 0.99 0.7593, time 7.84
Epoch 182, loss 0.07, acc 0.98 0.7608, time 7.83
Epoch 183, loss 0.01, acc 1.00 0.7659, time 7.82
Epoch 184, loss 0.06, acc 0.97 0.7656, time 7.83
Epoch 185, loss 0.03, acc 0.99 0.7633, time 7.84
Epoch 186, loss 0.06, acc 0.98 0.7626, time 7.83
Epoch 187, loss 0.02, acc 1.00 0.7589, time 7.87
Epoch 188, loss 0.14, acc 0.98 0.7600, time 8.05
Epoch 189, loss 0.03, acc 0.99 0.7582, time 7.95
Epoch 190, loss 0.08, acc 0.98 0.7593, time 7.91
Epoch 191, loss 0.04, acc 0.99 0.7578, time 7.85
Epoch 192, loss 0.08, acc 0.97 0.7575, time 7.87
Epoch 193, loss 0.05, acc 0.97 0.7541, time 9.62
Epoch 194, loss 0.03, acc 0.99 0.7575, time 8.09
Epoch 195, loss 0.05, acc 0.98 0.7582, time 7.97
Epoch 196, loss 0.06, acc 0.99 0.7578, time 7.86
Epoch 197, loss 0.02, acc 0.99 0.7615, time 7.85
Epoch 198, loss 0.01, acc 1.00 0.7541, time 8.49
Epoch 199, loss 0.09, acc 0.96 0.7563, time 8.35
Epoch 200, loss 0.10, acc 0.97 0.7556, time 8.77
Done training, best_step: 8720, best_acc: 0.7722
duration: 0.49 hours

accuracy: 0.7722
!!!WARNING!!! The proposed file contains 1 label(s) of type 'Entity-Destination(e2,e1)', which is NOT present in the key file.

<<< (2*9+1)-WAY EVALUATION (USING DIRECTIONALITY)>>>:

Confusion matrix:
        C-E1 C-E2 C-W1 C-W2 C-C1 C-C2 E-D1 E-O1 E-O2 I-A1 I-A2 M-C1 M-C2 M-T1 M-T2 P-P1 P-P2  _O_ *ED2 <-- classified as
      +-----------------------------------------------------------------------------------------------+ -SUM- skip ACTUAL
 C-E1 | 127    3    1    0    0    0    0    1    1    0    0    0    0    2    0    0    0   12    0 |  147    0  147
 C-E2 |   2  175    0    0    0    0    0    2    0    0    0    0    0    0    0    3    0   13    0 |  195    0  195
 C-W1 |   0    0  138    3    2    1    2    1    1    2    2    1    5    1    0    2    1   18    0 |  180    0  180
 C-W2 |   0    0    2  118    0    0    0    0    0    0   13    0    2    2    0    0    3   24    0 |  164    0  164
 C-C1 |   0    0    2    0  139    3   12    2    1    0    1    0    0    0    0    0    1   22    0 |  183    0  183
 C-C2 |   0    0    1    4    2   31    0    0    0    0    0    0    0    1    0    0    0    2    1 |   42    0   42
 E-D1 |   0    0    0    2    8    0  260    5    0    0    2    2    0    1    0    0    1   23    0 |  304    0  304
 E-O1 |   0    5    0    1    1    0    1  179    1    0    0    1    0    2    0    2    3   21    0 |  217    0  217
 E-O2 |   0    0    0    1    0    1    0    0   36    0    0    0    0    0    0    0    0    1    0 |   39    0   39
 I-A1 |   0    0    3    1    0    0    0    0    0    9    0    0    0    0    0    1    0    1    0 |   15    0   15
 I-A2 |   0    0    1    4    0    0    1    1    1    1   83    0    0    0    0    0    7   15    0 |  114    0  114
 M-C1 |   0    0    2    0    0    0    0    0    0    0    1   19    0    1    0    0    0    5    0 |   28    0   28
 M-C2 |   0    0    3    2    0    1    0    0    0    0    0    1  179    1    0    0    1   29    0 |  217    0  217
 M-T1 |   1    0    0    4    0    1    0    1    0    0    2    0    0  181    2    2    1   22    0 |  217    0  217
 M-T2 |   0    2    0    2    0    0    0    1    0    1    0    1    0    2   42    1    1   12    0 |   65    0   65
 P-P1 |   0    3    0    0    0    1    0    4    0    7    0    0    1    1    0   86    0   10    0 |  113    0  113
 P-P2 |   0    0    0    1    0    0    0    1    1    1   11    0    0    0    0    1   84   12    0 |  112    0  112
  _O_ |   4    6    9    7    1    0   15   13    5    1   19    7   14   15    7   10   20  212    0 |  365    0  365
      +-----------------------------------------------------------------------------------------------+
 -SUM-  134  194  162  150  153   39  291  211   47   22  134   32  201  210   51  108  123  454    1   2717    0 2717

Coverage = 2717/2717 = 100.00%
Accuracy (calculated for the above confusion matrix) = 2098/2717 = 77.22%
Accuracy (considering all skipped examples as Wrong) = 2098/2717 = 77.22%
Accuracy (considering all skipped examples as Other) = 2098/2717 = 77.22%

Results for the individual relations:
      Cause-Effect(e1,e2) :    P =  127/ 134 =  94.78%     R =  127/ 147 =  86.39%     F1 =  90.39%
      Cause-Effect(e2,e1) :    P =  175/ 194 =  90.21%     R =  175/ 195 =  89.74%     F1 =  89.97%
   Component-Whole(e1,e2) :    P =  138/ 162 =  85.19%     R =  138/ 180 =  76.67%     F1 =  80.70%
   Component-Whole(e2,e1) :    P =  118/ 150 =  78.67%     R =  118/ 164 =  71.95%     F1 =  75.16%
 Content-Container(e1,e2) :    P =  139/ 153 =  90.85%     R =  139/ 183 =  75.96%     F1 =  82.74%
 Content-Container(e2,e1) :    P =   31/  39 =  79.49%     R =   31/  42 =  73.81%     F1 =  76.54%
Entity-Destination(e1,e2) :    P =  260/ 291 =  89.35%     R =  260/ 304 =  85.53%     F1 =  87.39%
     Entity-Origin(e1,e2) :    P =  179/ 211 =  84.83%     R =  179/ 217 =  82.49%     F1 =  83.64%
     Entity-Origin(e2,e1) :    P =   36/  47 =  76.60%     R =   36/  39 =  92.31%     F1 =  83.72%
 Instrument-Agency(e1,e2) :    P =    9/  22 =  40.91%     R =    9/  15 =  60.00%     F1 =  48.65%
 Instrument-Agency(e2,e1) :    P =   83/ 134 =  61.94%     R =   83/ 114 =  72.81%     F1 =  66.94%
 Member-Collection(e1,e2) :    P =   19/  32 =  59.38%     R =   19/  28 =  67.86%     F1 =  63.33%
 Member-Collection(e2,e1) :    P =  179/ 201 =  89.05%     R =  179/ 217 =  82.49%     F1 =  85.65%
     Message-Topic(e1,e2) :    P =  181/ 210 =  86.19%     R =  181/ 217 =  83.41%     F1 =  84.78%
     Message-Topic(e2,e1) :    P =   42/  51 =  82.35%     R =   42/  65 =  64.62%     F1 =  72.41%
  Product-Producer(e1,e2) :    P =   86/ 108 =  79.63%     R =   86/ 113 =  76.11%     F1 =  77.83%
  Product-Producer(e2,e1) :    P =   84/ 123 =  68.29%     R =   84/ 112 =  75.00%     F1 =  71.49%
                   _Other :    P =  212/ 454 =  46.70%     R =  212/ 365 =  58.08%     F1 =  51.77%

Micro-averaged result (excluding Other):
P = 1886/2262 =  83.38%     R = 1886/2352 =  80.19%     F1 =  81.75%

MACRO-averaged result (excluding Other):
P =  78.69%	R =  77.48%	F1 =  77.73%



<<< (9+1)-WAY EVALUATION IGNORING DIRECTIONALITY >>>:

Confusion matrix:
         C-E  C-W  C-C  E-D  E-O  I-A  M-C  M-T  P-P  _O_ <-- classified as
      +--------------------------------------------------+ -SUM- skip ACTUAL
  C-E | 307    1    0    0    4    0    0    2    3   25 |  342    0  342
  C-W |   0  261    3    2    2   17    8    3    6   42 |  344    0  344
  C-C |   0    7  175   13    3    1    0    1    1   24 |  225    0  225
  E-D |   0    2    8  260    5    2    2    1    1   23 |  304    0  304
  E-O |   5    2    2    1  216    0    1    2    5   22 |  256    0  256
  I-A |   0    9    0    1    2   93    0    0    8   16 |  129    0  129
  M-C |   0    7    1    0    0    1  199    2    1   34 |  245    0  245
  M-T |   3    6    1    0    2    3    1  227    5   34 |  282    0  282
  P-P |   3    1    1    0    6   19    1    1  171   22 |  225    0  225
  _O_ |  10   16    1   15   18   20   21   22   30  212 |  365    0  365
      +--------------------------------------------------+
 -SUM-  328  312  192  292  258  156  233  261  231  454   2717    0 2717

Coverage = 2717/2717 = 100.00%
Accuracy (calculated for the above confusion matrix) = 2121/2717 = 78.06%
Accuracy (considering all skipped examples as Wrong) = 2121/2717 = 78.06%
Accuracy (considering all skipped examples as Other) = 2121/2717 = 78.06%

Results for the individual relations:
             Cause-Effect :    P =  307/ 328 =  93.60%     R =  307/ 342 =  89.77%     F1 =  91.64%
          Component-Whole :    P =  261/ 312 =  83.65%     R =  261/ 344 =  75.87%     F1 =  79.57%
        Content-Container :    P =  175/ 192 =  91.15%     R =  175/ 225 =  77.78%     F1 =  83.93%
       Entity-Destination :    P =  260/ 292 =  89.04%     R =  260/ 304 =  85.53%     F1 =  87.25%
            Entity-Origin :    P =  216/ 258 =  83.72%     R =  216/ 256 =  84.38%     F1 =  84.05%
        Instrument-Agency :    P =   93/ 156 =  59.62%     R =   93/ 129 =  72.09%     F1 =  65.26%
        Member-Collection :    P =  199/ 233 =  85.41%     R =  199/ 245 =  81.22%     F1 =  83.26%
            Message-Topic :    P =  227/ 261 =  86.97%     R =  227/ 282 =  80.50%     F1 =  83.61%
         Product-Producer :    P =  171/ 231 =  74.03%     R =  171/ 225 =  76.00%     F1 =  75.00%
                   _Other :    P =  212/ 454 =  46.70%     R =  212/ 365 =  58.08%     F1 =  51.77%

Micro-averaged result (excluding Other):
P = 1909/2263 =  84.36%     R = 1909/2352 =  81.16%     F1 =  82.73%

MACRO-averaged result (excluding Other):
P =  83.02%	R =  80.35%	F1 =  81.51%



<<< (9+1)-WAY EVALUATION TAKING DIRECTIONALITY INTO ACCOUNT -- OFFICIAL >>>:

Confusion matrix:
         C-E  C-W  C-C  E-D  E-O  I-A  M-C  M-T  P-P  _O_ <-- classified as
      +--------------------------------------------------+ -SUM- xDIRx skip  ACTUAL
  C-E | 302    1    0    0    4    0    0    2    3   25 |  337     5     0    342
  C-W |   0  256    3    2    2   17    8    3    6   42 |  339     5     0    344
  C-C |   0    7  170   13    3    1    0    1    1   24 |  220     5     0    225
  E-D |   0    2    8  260    5    2    2    1    1   23 |  304     0     0    304
  E-O |   5    2    2    1  215    0    1    2    5   22 |  255     1     0    256
  I-A |   0    9    0    1    2   92    0    0    8   16 |  128     1     0    129
  M-C |   0    7    1    0    0    1  198    2    1   34 |  244     1     0    245
  M-T |   3    6    1    0    2    3    1  223    5   34 |  278     4     0    282
  P-P |   3    1    1    0    6   19    1    1  170   22 |  224     1     0    225
  _O_ |  10   16    1   15   18   20   21   22   30  212 |  365     0     0    365
      +--------------------------------------------------+
 -SUM-  323  307  187  292  257  155  232  257  230  454   2694    23     0   2717

Coverage = 2717/2717 = 100.00%
Accuracy (calculated for the above confusion matrix) = 2098/2717 = 77.22%
Accuracy (considering all skipped examples as Wrong) = 2098/2717 = 77.22%
Accuracy (considering all skipped examples as Other) = 2098/2717 = 77.22%

Results for the individual relations:
             Cause-Effect :    P =  302/( 323 +   5) =  92.07%     R =  302/ 342 =  88.30%     F1 =  90.15%
          Component-Whole :    P =  256/( 307 +   5) =  82.05%     R =  256/ 344 =  74.42%     F1 =  78.05%
        Content-Container :    P =  170/( 187 +   5) =  88.54%     R =  170/ 225 =  75.56%     F1 =  81.53%
       Entity-Destination :    P =  260/( 292 +   0) =  89.04%     R =  260/ 304 =  85.53%     F1 =  87.25%
            Entity-Origin :    P =  215/( 257 +   1) =  83.33%     R =  215/ 256 =  83.98%     F1 =  83.66%
        Instrument-Agency :    P =   92/( 155 +   1) =  58.97%     R =   92/ 129 =  71.32%     F1 =  64.56%
        Member-Collection :    P =  198/( 232 +   1) =  84.98%     R =  198/ 245 =  80.82%     F1 =  82.85%
            Message-Topic :    P =  223/( 257 +   4) =  85.44%     R =  223/ 282 =  79.08%     F1 =  82.14%
         Product-Producer :    P =  170/( 230 +   1) =  73.59%     R =  170/ 225 =  75.56%     F1 =  74.56%
                   _Other :    P =  212/( 454 +   0) =  46.70%     R =  212/ 365 =  58.08%     F1 =  51.77%

Micro-averaged result (excluding Other):
P = 1886/2263 =  83.34%     R = 1886/2352 =  80.19%     F1 =  81.73%

MACRO-averaged result (excluding Other):
P =  82.00%	R =  79.40%	F1 =  80.53%



<<< The official score is (9+1)-way evaluation with directionality taken into account: macro-averaged F1 = 80.53% >>>